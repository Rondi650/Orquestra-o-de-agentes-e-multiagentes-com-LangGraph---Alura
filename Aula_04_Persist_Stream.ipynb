{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde6402",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install -U langgraph langchain langchain-google-genai langchain-tavily python-dotenv aiosqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14429a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GEMINI_API_KEY') \n",
    "os.environ['TAVILY_API_KEY'] = os.getenv('TAVILY_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfcef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Any, Dict\n",
    "from dataclasses import dataclass, field\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage, BaseMessage, AnyMessage\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263303b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "\n",
    "conn = sqlite3.connect(\"checkpoints.db\", check_same_thread=False)\n",
    "memory = SqliteSaver(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed8cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        \n",
    "        graph = StateGraph(AgentState)\n",
    "\n",
    "        graph.add_node(\"llm\", self.call_gemini)\n",
    "\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "\n",
    "        graph.set_entry_point(\"llm\")\n",
    "\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_gemini(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "\n",
    "        print(\"Mensagens enviadas ao modelo:\", messages)\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling Tool: {t['name']} with args: {t['args']}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Returning to LLM after action!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc332fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_tavily_api_key = os.getenv('TAVILY_API_KEY')\n",
    "if not current_tavily_api_key:\n",
    "    raise ValueError(\"TAVILY_API_KEY não encontrada. Certifique-se de que está no seu .env e python-dotenv está instalado.\")\n",
    "\n",
    "tool = TavilySearch(max_results=3, tavily_api_key=current_tavily_api_key)\n",
    "\n",
    "\n",
    "prompt_system = \"\"\"Você é um assistente de pesquisa inteligente. Use o mecanismo de busca (tavily_search_results_json) para procurar informações.\n",
    "Você tem permissão para fazer múltiplas chamadas à ferramenta (em conjunto ou em sequência).\n",
    "Busque informações apenas quando tiver certeza do que procurar.\n",
    "Se precisar de mais detalhes para formular uma pergunta de acompanhamento, você tem permissão para fazer isso.\n",
    "Quando solicitado a comparar informações (ex: qual é mais quente, maior, etc.), use as informações do histórico da conversa e dos resultados das ferramentas.\"\"\"\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\") \n",
    "\n",
    "abot = Agent(model, [tool], system=prompt_system, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd1df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Como está o tempo em São Paulo hoje (21/08/2025)?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}} \n",
    "\n",
    "\n",
    "print(\"\\n--- Pergunta 1: Tempo em São Paulo ---\")\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for k, v in event.items():\n",
    "        if k in (\"llm\", \"action\"): \n",
    "             print(f\"{k}: {v['messages']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd24b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"E no Rio de Janeiro?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}} \n",
    "\n",
    "print(\"\\n--- Pergunta 2: Tempo no Rio de Janeiro ---\")\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for k, v in event.items():\n",
    "        if k in (\"llm\", \"action\"):\n",
    "            print(f\"{k}: {v['messages']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154f9788",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Qual está mais quente?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}} \n",
    "\n",
    "print(\"\\n--- Pergunta 3: Comparação ---\")\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for k, v in event.items():\n",
    "        if k in (\"llm\", \"action\"):\n",
    "            print(f\"{k}: {v['messages']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68285b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Qual está mais quente?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}} \n",
    "\n",
    "print(\"\\n--- Pergunta 4: Comparação ---\")\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for k, v in event.items():\n",
    "        if k in (\"llm\", \"action\"):\n",
    "            print(f\"{k}: {v['messages']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
