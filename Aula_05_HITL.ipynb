{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3037c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-google-genai langchain_community tavily-python aiosqlite langchain-tavily \n",
    "%pip install langgraph-checkpoint-sqlite \n",
    "%pip install langgraph langgraph-checkpoint-sqlite\n",
    "%pip install langgraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List, Any, Dict\n",
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage, AIMessage, BaseMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_tavily import TavilySearch\n",
    "import sqlite3\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "conn = sqlite3.connect(\"checkpoints.db\", check_same_thread=False)\n",
    "memory = SqliteSaver(conn)\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c908db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GEMINI_API_KEY') \n",
    "os.environ['TAVILY_API_KEY'] = os.getenv('TAVILY_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d789b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apenas exemplo - não precisa rodar\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae247748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "def reduce_messages(left: list[AnyMessage], right: list[AnyMessage]) -> list[AnyMessage]:\n",
    "\n",
    "    for message in right:\n",
    "        if not message.id:\n",
    "            message.id = str(uuid4())\n",
    "    \n",
    "    merged = left.copy()\n",
    "    for message in right:\n",
    "        for i, existing in enumerate(merged):\n",
    "    \n",
    "            if existing.id == message.id:\n",
    "                merged[i] = message\n",
    "                break\n",
    "        else:\n",
    "    \n",
    "            merged.append(message)\n",
    "    return merged\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], reduce_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d35fb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-community tavily-python\n",
    "%pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39473c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c810c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import SystemMessage, ToolMessage\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, model, tools, system=\"\", checkpointer=None):\n",
    "        self.system = system\n",
    "\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_gemini)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(\n",
    "            checkpointer=checkpointer,\n",
    "            interrupt_before=[\"action\"] # Adiciona interrupção antes de chamar a ação\n",
    "        )\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_gemini(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        print(state)\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Chamando ferramenta: {t['name']} com argumentos: {t['args']}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Voltando para o modelo!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bdfaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "current_date = date.today().strftime(\"%d/%m/%Y\") \n",
    "\n",
    "prompt = f\"\"\"Você é um assistente de pesquisa inteligente e altamente atualizado. \\\n",
    "Sua principal prioridade é encontrar as informações mais RECENTES e em TEMPO REAL sempre que possível. \\\n",
    "A data atual é {current_date}. \\\n",
    "Ao buscar sobre o tempo ou eventos que se referem a \"hoje\" ou \"agora\", \\\n",
    "você DEVE **incluir a data atual '{current_date}' na sua consulta para a ferramenta de busca**. \\\n",
    "Por exemplo, se a pergunta é \"tempo em cidade x hoje\", a consulta para a ferramenta deve ser \"tempo em cidade x {current_date}\". \\\n",
    "Ignore ou descarte informações que claramente se refiram a datas passadas ou futuras ao responder perguntas sobre \"hoje\". \\\n",
    "Use o mecanismo de busca para procurar informações, sempre buscando o 'hoje' ou o 'agora' quando o contexto indicar. \\\n",
    "Você tem permissão para fazer múltiplas chamadas (seja em conjunto ou em sequência). \\\n",
    "Procure informações apenas quando tiver certeza do que você quer. \\\n",
    "Se precisar pesquisar alguma informação antes de fazer uma pergunta de acompanhamento, você tem permissão para fazer isso!\n",
    "\"\"\"\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15436a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "dynamic_thread_id = str(uuid.uuid4())\n",
    "\n",
    "print(f\"Meu novo Thread ID dinâmico é: {dynamic_thread_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c550fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = str(uuid.uuid4())\n",
    "print(f\"DEBUG: Iniciando nova conversa com ID: {session_id}\\n\")\n",
    "\n",
    "user_message = \"Como está o tempo em São Paulo hoje?\"\n",
    "messages = [HumanMessage(content=user_message)]\n",
    "thread_config = {\"configurable\": {\"thread_id\": session_id}}\n",
    "\n",
    "print(\"--- Etapa 1: Agente processa a entrada e decide a ação ---\")\n",
    "print(f\"Você: {user_message}\")\n",
    "\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread_config):\n",
    "\n",
    "    for k, v in event.items():\n",
    "        if k == \"llm\":\n",
    "            last_message = v.get('messages', [])[-1]\n",
    "            if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "                print(f\"\\nAgente (decisão): {last_message.tool_calls}\")\n",
    "                print(\"\\n--- AGENTE PAUSADO: Intervenção Humana Necessária ---\")\n",
    "            else:\n",
    "                print(f\"\\nAgente (resposta direta/sem tool_calls): {last_message.content}\")\n",
    "                print(\"\\n--- AGENTE PAUSADO (resposta direta, sem ação pendente) ---\")\n",
    "\n",
    "current_state = abot.graph.get_state(thread_config)\n",
    "\n",
    "last_state_message = current_state.values['messages'][-1]\n",
    "\n",
    "if current_state and current_state.next == ('action',) and isinstance(last_state_message, AIMessage) and last_state_message.tool_calls:\n",
    "    tool_calls_pending = last_state_message.tool_calls\n",
    "    if tool_calls_pending:\n",
    "        print(\"\\nO agente decidiu executar a(s) seguinte(s) ação(ões) de ferramenta:\")\n",
    "        for tc in tool_calls_pending:\n",
    "            print(f\"- Ferramenta: {tc['name']}, Argumentos: {tc['args']}\")\n",
    "\n",
    "        user_input = input(\"\\nVocê deseja que o agente execute esta(s) ação(ões)? (sim/não): \").lower()\n",
    "\n",
    "        if user_input == 'sim':\n",
    "            \n",
    "            print(\"\\n--- Etapa 2: Retomando a execução (Agente executará a ação) ---\")\n",
    "            for event in abot.graph.stream(None, thread_config):\n",
    "                for k, v in event.items():\n",
    "                    if k == \"action\":\n",
    "                        print(f\"DEBUG: Ferramenta executada e resultado retornado: {v}\")\n",
    "                    elif k == \"llm\":\n",
    "                        final_response_message = v.get('messages', [])[-1].content\n",
    "                        print(f\"\\nAgente (resposta final): {final_response_message}\")\n",
    "                    elif k == END:\n",
    "                        print(f\"DEBUG: Grafo terminou a execução.\")\n",
    "            print(\"\\n--- FIM DA INTERAÇÃO ---\")\n",
    "        else:\n",
    "            print(\"\\nExecução da ação cancelada pelo usuário.\")\n",
    "            print(\"--- FIM DA INTERAÇÃO ---\")\n",
    "    else:\n",
    "        print(\"\\nO agente não decidiu nenhuma ação de ferramenta apesar da pausa. Interação encerrada.\")\n",
    "else:\n",
    "    print(\"\\nO agente respondeu diretamente ou não pausou em uma ação. Não há ações pendentes para aprovar.\")\n",
    "    if current_state:\n",
    "        final_response_message = current_state.values['messages'][-1].content\n",
    "        print(f\"Agente (resposta direta): {final_response_message}\")\n",
    "    print(\"--- FIM DA INTERAÇÃO ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9680763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"\\n--- Tentando Gerar PNG do Grafo via Mermaid ---\")\n",
    "try:\n",
    "    image_data = abot.graph.get_graph().draw_mermaid_png()\n",
    "    display(Image(data=image_data))\n",
    "\n",
    "except AttributeError:\n",
    "    print(\"Método `.draw_mermaid_png()` não encontrado ou não suportado.\")\n",
    "    print(\"Tentando gerar apenas o código Mermaid...\")\n",
    "    try:\n",
    "        mermaid_code = abot.graph.get_graph().draw_mermaid()\n",
    "        print(\"\\n--- Código Mermaid Gerado (Cole em https://mermaid.live/) ---\")\n",
    "        print(mermaid_code)\n",
    "    except Exception as e_mermaid:\n",
    "        print(f\"Erro ao gerar código Mermaid: {e_mermaid}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro inesperado ao tentar gerar o grafo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ccd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import uuid\n",
    "\n",
    "new_session_id = str(uuid.uuid4())\n",
    "print(f\"DEBUG: Iniciando nova conversa com ID: {new_session_id}\\n\")\n",
    "new_user_message = \"Qual é a distância entre o Rio de Janeiro e Tóquio?\"\n",
    "new_messages = [HumanMessage(content=new_user_message)]\n",
    "new_thread_config = {\"configurable\": {\"thread_id\": new_session_id}}\n",
    "\n",
    "print(\"--- Iniciando NOVA Interação: Agente processa a entrada e decide a ação ---\")\n",
    "print(f\"Você: {new_user_message}\")\n",
    "print(f\"DEBUG: Nova Thread ID: {new_session_id}\")\n",
    "\n",
    "print(\"\\n--- Agente pensando e pausando ---\")\n",
    "try:\n",
    "    for event in abot.graph.stream({\"messages\": new_messages}, new_thread_config):\n",
    "        for k, v in event.items():\n",
    "            if k == \"llm\":\n",
    "                if v and 'messages' in v and v['messages']:\n",
    "                    llm_message_from_event = v['messages'][0]\n",
    "                    if hasattr(llm_message_from_event, 'tool_calls') and llm_message_from_event.tool_calls:\n",
    "                        print(f\"\\nAgente (decisão): {llm_message_from_event.tool_calls}\")\n",
    "                        print(\"\\n--- AGENTE PAUSADO: Intervenção Humana Necessária ---\")\n",
    "                        \n",
    "                    elif llm_message_from_event.content:\n",
    "                        print(f\"\\nAgente (resposta direta): {llm_message_from_event.content}\")\n",
    "                        print(\"\\n--- AGENTE NÃO PAUSOU PARA FERRAMENTA (Resposta direta do LLM) ---\")\n",
    "except Exception as e:\n",
    "    print(f\"DEBUG: Stream interrompido como esperado: {e}\")\n",
    "\n",
    "current_state_snapshot = abot.graph.get_state(new_thread_config)\n",
    "\n",
    "if current_state_snapshot:\n",
    "    print(f\"\\nDEBUG: Estado atual obtido para NOVA thread ID: {new_session_id}\")\n",
    "    \n",
    "    snapshot_thread_id = None\n",
    "    snapshot_thread_ts = None\n",
    "\n",
    "    if hasattr(current_state_snapshot, 'config') and isinstance(current_state_snapshot.config, dict):\n",
    "        if 'configurable' in current_state_snapshot.config and isinstance(current_state_snapshot.config['configurable'], dict):\n",
    "            if 'thread_id' in current_state_snapshot.config['configurable']:\n",
    "                snapshot_thread_id = current_state_snapshot.config['configurable']['thread_id']\n",
    "            if '__run_id' in current_state_snapshot.config['configurable']:\n",
    "                snapshot_thread_ts = current_state_snapshot.config['configurable']['__run_id']\n",
    "            elif 'thread_ts' in current_state_snapshot.config['configurable']:\n",
    "                snapshot_thread_ts = current_state_snapshot.config['configurable']['thread_ts']\n",
    "    \n",
    "    if snapshot_thread_id is None:\n",
    "        snapshot_thread_id = new_session_id\n",
    "\n",
    "    print(f\"DEBUG: ID da Thread (do snapshot): {snapshot_thread_id}\")\n",
    "    print(f\"DEBUG: Timestamp do snapshot (thread_ts): {snapshot_thread_ts}\") \n",
    "    print(f\"DEBUG: Mensagens no snapshot (no momento da pausa): {current_state_snapshot.values.get('messages')}\")\n",
    "    \n",
    "    if current_state_snapshot.values and 'messages' in current_state_snapshot.values:\n",
    "        last_msg_in_snapshot = current_state_snapshot.values['messages'][-1]\n",
    "        print(f\"DEBUG: Tipo da última mensagem no snapshot para injeção: {type(last_msg_in_snapshot)}\")\n",
    "        if hasattr(last_msg_in_snapshot, 'tool_calls') and last_msg_in_snapshot.tool_calls:\n",
    "            print(f\"DEBUG: Última mensagem no snapshot TEM tool_calls. PRONTO PARA INJEÇÃO!\")\n",
    "        else:\n",
    "            print(f\"DEBUG: Última mensagem no snapshot NÃO TEM tool_calls ou está vazia. PROBLEMA NA PAUSA!\")\n",
    "    if current_state_snapshot.next != ():\n",
    "        print(\"\\n--- Agente está PAUSADO e pronto para intervenção. ---\")\n",
    "    else:\n",
    "        print(\"\\n--- ATENÇÃO: O agente NÃO está pausado onde esperávamos. O grafo pode ter terminado. ---\")\n",
    "else:\n",
    "    print(f\"DEBUG: Nenhum estado encontrado para a nova thread ID: {new_session_id}. Verifique a configuração da thread ou se o agente pausou.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06199d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "if current_state_snapshot:\n",
    "    modified_state_values = current_state_snapshot.values.copy()\n",
    "\n",
    "    final_injected_message = AIMessage(\n",
    "        content=\"A distância entre o Rio de Janeiro e Tóquio é de aproximadamente 450 km. (Dados fornecidos MANULMENTE por você!)\"\n",
    "    )\n",
    "    ai_message_found = False\n",
    "    for i, msg in enumerate(modified_state_values['messages']):\n",
    "        \n",
    "        if isinstance(msg, AIMessage):\n",
    "            modified_state_values['messages'] = modified_state_values['messages'][:i] + [final_injected_message]\n",
    "            ai_message_found = True\n",
    "            break\n",
    "            \n",
    "    if not ai_message_found:\n",
    "        modified_state_values['messages'].append(final_injected_message)\n",
    "\n",
    "    print(\"\\n--- Estado sendo MODIFICADO MANUALMENTE (Injetando AIMessage Final) ---\")\n",
    "    print(f\"DEBUG: Conteúdo da AIMessage falsa injetada: {final_injected_message.content}\")\n",
    "    print(f\"DEBUG: Nova lista de mensagens (últimas): {[m.type for m in modified_state_values['messages'][-2:]]}\")\n",
    "\n",
    "else:\n",
    "    print(\"DEBUG: Não é possível modificar o estado porque nenhum snapshot do estado foi encontrado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Finalizando o estado com a resposta injetada ---\")\n",
    "\n",
    "abot.graph.update_state(new_thread_config, modified_state_values)\n",
    "\n",
    "final_state_after_injection_obj = abot.graph.get_state(new_thread_config)\n",
    "\n",
    "print(\"\\n--- Saída final do agente após intervenção ---\")\n",
    "\n",
    "if hasattr(final_state_after_injection_obj, 'values') and isinstance(final_state_after_injection_obj.values, dict):\n",
    "    final_messages = final_state_after_injection_obj.values['messages']\n",
    "elif isinstance(final_state_after_injection_obj, dict):\n",
    "    final_messages = final_state_after_injection_obj['messages']\n",
    "\n",
    "else:\n",
    "    found_messages_list = None\n",
    "    if isinstance(final_state_after_injection_obj, tuple):\n",
    "        for item in final_state_after_injection_obj:\n",
    "            if isinstance(item, dict) and 'messages' in item:\n",
    "                found_messages_list = item['messages']\n",
    "                break\n",
    "    elif isinstance(final_state_after_injection_obj, dict) and 'messages' in final_state_after_injection_obj:\n",
    "        found_messages_list = final_state_after_injection_obj['messages']\n",
    "    \n",
    "    if found_messages_list is not None:\n",
    "        final_messages = found_messages_list\n",
    "    else:\n",
    "        print(f\"DEBUG: Não foi possível extrair a lista de mensagens do objeto de estado final: {final_state_after_injection_obj}\")\n",
    "        final_messages = []\n",
    "\n",
    "if final_messages and isinstance(final_messages[-1], AIMessage):\n",
    "    print(f\"\\nAgente: {final_messages[-1].content}\")\n",
    "else:\n",
    "    print(\"\\nAgente: Resposta final não encontrada ou não é um AIMessage.\")\n",
    "    print(f\"DEBUG: Estado final completo (para inspeção): {final_state_after_injection_obj}\")\n",
    "\n",
    "print(\"\\n--- Fluxo de Human-in-the-Loop concluído ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
